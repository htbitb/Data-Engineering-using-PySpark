{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and initialize function from pyyparksql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `SparkSession` is the entry point to programming Spark with the Dataset and DataFrame API. Itâ€™s the new entry point that supersedes the old `SparkContext`, `SQLContext`, and `HiveContext`. `SparkSession` provides a unified interface for interacting with Spark.\n",
    "* The `builder` method is used to create a `SparkSession`.Builder object, which provides various options for configuring and creating a SparkSession. The `builder` method is a static method, so it is called on the SparkSession class itself.\n",
    "* The `appName` method sets a name for the application. This name will appear in the Spark UI and can be useful for identifying your application. In this case, the application name is set to \"SparkSQL\". The appName method is called on the SparkSession.Builder object.\n",
    "* The `getOrCreate` method is used to either get an existing `SparkSession` or, if none exists, create a new one. This is useful for ensuring that there is only one SparkSession per application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined mapper() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), \\\n",
    "               age=int(fields[2]), numFriends=int(fields[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a line of text, splits it by `commas`, and returns a `Row` object with fields `ID`, `name`, `age`, and `numFriends`. The Row object helps define the schema for the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Mapping Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"./source/fakefriends.csv\")\n",
    "people = lines.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `spark.sparkContext.textFile(\"./source/fakefriends.csv\")`: Reads the CSV file into an RDD called lines.\n",
    "* `lines.map(mapper)`: Applies the mapper function to each line, converting it to a Row object. This results in an RDD of Row objects called people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame and Registering as a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `spark.createDataFrame(people).cache()`: Converts the people RDD to a DataFrame and caches it for faster access.\n",
    "* `schemaPeople.createOrReplaceTempView(\"people\")`: Registers the DataFrame as a temporary table named \"people\". This allows SQL queries to be run against it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "  print(teen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")`: Executes an SQL query to select rows where age is between 13 and 19, inclusive. The result is a DataFrame named teenagers.\n",
    "* `teenagers.collect()`: Collects the results of the query back to the driver program as a list of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing data operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use functions instead of SQL queries:\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `schemaPeople.groupBy(\"age\").count()`: Groups the DataFrame by the age column and counts the number of rows for each age.\n",
    "* `orderBy(\"age\").show()`: Orders the results by the age column and displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "# Stop the Spark session to release resourcess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
