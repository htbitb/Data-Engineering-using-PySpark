{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview about `flatmap`\n",
    "\n",
    "##### Map()\n",
    "The `map()` transformation applies a given function to each element of the RDD and returns a new RDD containing the results of these function applications. Each input element is transformed into exactly one output element.\n",
    "\n",
    "**Characteristics:**\n",
    "* One-to-One Transformation: Each element in the input RDD is mapped to one element in the output RDD.\n",
    "* Output RDD Size: The output RDD has the same number of elements as the input RDD.\n",
    "\n",
    "##### flatmap()\n",
    "The `flatMap()` transformation applies a given function to each element of the RDD and returns a new RDD by flattening the results. The function can return multiple output elements for each input element (typically a list or another iterable), and flatMap() will flatten these lists into a single RDD.\n",
    "\n",
    "**Characteristics**:\n",
    "* One-to-Many Transformation: Each element in the input RDD can be mapped to zero or more elements in the output RDD.\n",
    "* Output RDD Size: The output RDD can have more or fewer elements than the input RDD.\n",
    "\n",
    "##### Examples\n",
    "We have a RDD:\n",
    "```python\n",
    "rdd = sc.parallelize([\"hello world\", \"apache spark\"])\n",
    "```\n",
    "\n",
    "If use map():\n",
    "```python\n",
    "mapped_rdd = rdd.map(lambda sentence: sentence.split(\" \"))\n",
    "print(mapped_rdd.collect())\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "[['hello', 'world'], ['apache', 'spark']]\n",
    "```\n",
    "\n",
    "\n",
    "if use flatmap():\n",
    "```python\n",
    "flat_mapped_rdd = rdd.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "print(flat_mapped_rdd.collect())\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['hello', 'world', 'apache', 'spark']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define libraries\n",
    "* `re`: Python's regular expression library for string manipulation.\n",
    "* `SparkConf` and `SparkContext` from pyspark for Spark configuration and context creation.\n",
    "\n",
    "#### 2. Define normalized functions\n",
    "* `normalizeWords(text)`: This function takes a string (text) as input.\n",
    "* `re.compile(r'\\W+', re.UNICODE).split(text.lower())`:\n",
    "  * `re.compile(r'\\W+', re.UNICODE)`: Compiles a regular expression pattern that matches one or more non-word characters (anything other than a letter, digit, or underscore).\n",
    "  * `.split(text.lower())`: Splits the text into words based on the compiled regular expression, after converting all characters to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and SparkContext Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCountExplicit\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Configuring Spark:\n",
    "\n",
    "* `SparkConf().setMaster(\"local\")`: Configures Spark to run locally with a single thread.\n",
    "* `setAppName(\"WordCountExplicit\")`: Names the Spark application \"WordCount\".\n",
    "\n",
    "##### 4. Creating SparkContext:\n",
    "\n",
    "* `sc = SparkContext(conf = conf)`: Initializes the Spark context with the specified configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Input Data and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sc.textFile(\"./book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "wordCounts = words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Loading Data:\n",
    "\n",
    "* `sc.textFile(\"./book.txt\")`: Reads the text file located at the specified path and creates an RDD called input. Each element of this RDD is a line from the text file.\n",
    "\n",
    "##### 6. FlatMap Transformation:\n",
    "\n",
    "* `input.flatMap(normalizeWords):`\n",
    "  * `flatMap` applies the `normalizeWords` function to each line of the input RDD.\n",
    "  * The `normalizeWords` function splits each line into words, converting all characters to lowercase and splitting based on non-word characters.\n",
    "  * The resulting RDD (`words`) contains all words as individual elements, flattened into a single RDD.\n",
    "\n",
    "##### 7. Counting Words:\n",
    "\n",
    "* `words.countByValue():`\n",
    "  * countByValue counts the occurrences of each word in the words RDD.\n",
    "  * It returns a dictionary where the keys are words and the values are their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
